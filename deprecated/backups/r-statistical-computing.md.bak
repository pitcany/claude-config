---
name: r-statistical-computing
description: R programming for statistical analysis, data visualization with ggplot2, tidyverse data manipulation, and statistical modeling. Use when performing statistical analysis, creating R visualizations, data wrangling with dplyr/tidyr, or building statistical models.
---

# R Statistical Computing Skill

You are an R programming expert specializing in statistical analysis, data visualization, and the tidyverse ecosystem.

## Purpose

This skill provides expertise in:
- Tidyverse data manipulation (dplyr, tidyr)
- Data visualization with ggplot2
- Statistical modeling and inference
- Time series analysis
- Machine learning with caret/tidymodels
- R Markdown reports
- Performance optimization

## Core Tidyverse Patterns

### Data Manipulation with dplyr

```r
library(tidyverse)

# Pipe operator for readable code
result <- data %>%
  filter(age > 18, status == "active") %>%
  select(id, name, age, revenue) %>%
  mutate(
    revenue_k = revenue / 1000,
    age_group = case_when(
      age < 30 ~ "young",
      age < 50 ~ "middle",
      TRUE ~ "senior"
    )
  ) %>%
  group_by(age_group) %>%
  summarise(
    count = n(),
    avg_revenue = mean(revenue, na.rm = TRUE),
    median_revenue = median(revenue, na.rm = TRUE),
    total_revenue = sum(revenue, na.rm = TRUE)
  ) %>%
  arrange(desc(total_revenue))

# Window functions
data %>%
  group_by(category) %>%
  mutate(
    rank = row_number(desc(sales)),
    pct_of_category = sales / sum(sales) * 100,
    running_total = cumsum(sales),
    moving_avg = zoo::rollmean(sales, k = 7, fill = NA, align = "right")
  )
```

### Data Reshaping with tidyr

```r
# Wide to long format
data_long <- data_wide %>%
  pivot_longer(
    cols = starts_with("sales_"),
    names_to = "month",
    values_to = "sales",
    names_prefix = "sales_"
  )

# Long to wide format
data_wide <- data_long %>%
  pivot_wider(
    names_from = month,
    values_from = sales,
    names_prefix = "sales_"
  )

# Separate and unite columns
data %>%
  separate(full_name, into = c("first", "last"), sep = " ") %>%
  unite(full_address, street, city, state, sep = ", ")

# Handle nested data
nested_data <- data %>%
  group_by(category) %>%
  nest() %>%
  mutate(
    model = map(data, ~lm(sales ~ time, data = .)),
    predictions = map2(model, data, predict)
  )
```

## Data Visualization with ggplot2

### Basic Plot Structure

```r
library(ggplot2)

# The grammar of graphics
ggplot(data, aes(x = date, y = sales, color = category)) +
  geom_line(size = 1.2) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_brewer(palette = "Set2") +
  labs(
    title = "Sales Trends by Category",
    subtitle = "January - December 2024",
    x = "Date",
    y = "Sales ($)",
    color = "Category",
    caption = "Source: PitcanAnalytics"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    legend.position = "bottom",
    panel.grid.minor = element_blank()
  )
```

### Advanced Visualizations

```r
# Faceting
ggplot(data, aes(x = value, fill = category)) +
  geom_histogram(bins = 30, alpha = 0.7) +
  facet_wrap(~region, scales = "free") +
  theme_bw()

# Multiple geoms and annotations
ggplot(data, aes(x = date, y = value)) +
  geom_line(color = "steelblue", size = 1) +
  geom_smooth(method = "loess", se = TRUE, alpha = 0.2) +
  geom_vline(xintercept = as.Date("2024-06-01"), 
             linetype = "dashed", color = "red") +
  annotate("text", x = as.Date("2024-06-15"), y = max(data$value),
           label = "Product Launch", hjust = 0) +
  theme_minimal()

# Box plots with points
ggplot(data, aes(x = category, y = value, fill = category)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.3, size = 2) +
  stat_summary(fun = mean, geom = "point", 
               shape = 23, size = 3, fill = "red") +
  theme_classic()

# Heatmap
ggplot(correlation_data, aes(x = var1, y = var2, fill = correlation)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red",
                       midpoint = 0, limits = c(-1, 1)) +
  geom_text(aes(label = round(correlation, 2)), size = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Custom Themes

```r
# Create custom theme
theme_pitcan <- function() {
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0),
    plot.subtitle = element_text(size = 12, color = "gray40"),
    axis.title = element_text(face = "bold"),
    legend.position = "bottom",
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "gray90"),
    plot.background = element_rect(fill = "white", color = NA)
  )
}

# Use custom theme
ggplot(data, aes(x, y)) +
  geom_point() +
  theme_pitcan()
```

## Statistical Analysis

### Descriptive Statistics

```r
library(psych)

# Summary statistics
data %>%
  select(where(is.numeric)) %>%
  describe()

# Group-wise statistics
data %>%
  group_by(category) %>%
  summarise(
    n = n(),
    mean = mean(value, na.rm = TRUE),
    sd = sd(value, na.rm = TRUE),
    median = median(value, na.rm = TRUE),
    q25 = quantile(value, 0.25, na.rm = TRUE),
    q75 = quantile(value, 0.75, na.rm = TRUE),
    iqr = IQR(value, na.rm = TRUE)
  )

# Correlation matrix
cor_matrix <- data %>%
  select(where(is.numeric)) %>%
  cor(use = "complete.obs")

# Visualize correlation
library(corrplot)
corrplot(cor_matrix, method = "circle", type = "upper",
         tl.col = "black", tl.srt = 45)
```

### Hypothesis Testing

```r
# T-test
t.test(value ~ group, data = data)

# One-sample t-test
t.test(data$value, mu = 100)

# Paired t-test
t.test(before, after, paired = TRUE)

# ANOVA
model <- aov(value ~ category + region, data = data)
summary(model)

# Post-hoc tests
TukeyHSD(model)

# Chi-square test
chisq.test(table(data$category, data$outcome))

# Wilcoxon test (non-parametric)
wilcox.test(value ~ group, data = data)

# Kruskal-Wallis test
kruskal.test(value ~ category, data = data)
```

### Linear Regression

```r
# Simple linear regression
model <- lm(sales ~ advertising, data = data)
summary(model)

# Multiple regression
model <- lm(sales ~ advertising + price + competition, data = data)
summary(model)

# Model diagnostics
par(mfrow = c(2, 2))
plot(model)

# Predictions
new_data <- data.frame(
  advertising = c(100, 150, 200),
  price = c(29.99, 34.99, 39.99),
  competition = c(3, 4, 5)
)
predictions <- predict(model, new_data, interval = "confidence")

# Model comparison
model1 <- lm(sales ~ advertising, data = data)
model2 <- lm(sales ~ advertising + price, data = data)
anova(model1, model2)
```

### Logistic Regression

```r
# Binary logistic regression
model <- glm(outcome ~ age + income + education,
             data = data,
             family = binomial(link = "logit"))
summary(model)

# Odds ratios
exp(coef(model))
exp(confint(model))

# Predictions
data$predicted_prob <- predict(model, type = "response")
data$predicted_class <- ifelse(data$predicted_prob > 0.5, 1, 0)

# Model evaluation
library(pROC)
roc_obj <- roc(data$outcome, data$predicted_prob)
auc(roc_obj)
plot(roc_obj, main = "ROC Curve")

# Confusion matrix
table(Predicted = data$predicted_class, Actual = data$outcome)
```

## Machine Learning with tidymodels

### Model Workflow

```r
library(tidymodels)

# Split data
set.seed(123)
data_split <- initial_split(data, prop = 0.8, strata = outcome)
train_data <- training(data_split)
test_data <- testing(data_split)

# Create recipe for preprocessing
recipe <- recipe(outcome ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_corr(all_numeric_predictors(), threshold = 0.9)

# Specify model
rf_spec <- rand_forest(trees = 500, mtry = tune(), min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity")

# Create workflow
rf_workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(rf_spec)

# Hyperparameter tuning
set.seed(234)
folds <- vfold_cv(train_data, v = 5, strata = outcome)

rf_tuned <- rf_workflow %>%
  tune_grid(
    resamples = folds,
    grid = 20,
    metrics = metric_set(accuracy, roc_auc, sensitivity, specificity)
  )

# Best parameters
best_params <- select_best(rf_tuned, metric = "roc_auc")

# Final model
final_model <- rf_workflow %>%
  finalize_workflow(best_params) %>%
  fit(train_data)

# Evaluate on test set
predictions <- predict(final_model, test_data, type = "prob") %>%
  bind_cols(test_data)

metrics <- predictions %>%
  roc_auc(truth = outcome, .pred_1)
```

## Time Series Analysis

### Time Series Basics

```r
library(forecast)
library(tsibble)
library(feasts)

# Create time series object
ts_data <- ts(data$value, start = c(2020, 1), frequency = 12)

# Decomposition
decomp <- stl(ts_data, s.window = "periodic")
autoplot(decomp)

# ACF and PACF
acf(ts_data)
pacf(ts_data)

# Stationarity test
library(tseries)
adf.test(ts_data)
kpss.test(ts_data)
```

### Forecasting

```r
# Auto ARIMA
model <- auto.arima(ts_data)
summary(model)

# Forecast
forecast_result <- forecast(model, h = 12)
autoplot(forecast_result)

# Prophet (Facebook's forecasting tool)
library(prophet)

# Prepare data
df <- data.frame(
  ds = data$date,
  y = data$value
)

# Fit model
m <- prophet(df, yearly.seasonality = TRUE, weekly.seasonality = FALSE)

# Make forecast
future <- make_future_dataframe(m, periods = 365)
forecast <- predict(m, future)

# Plot
plot(m, forecast)
prophet_plot_components(m, forecast)

# Cross-validation
df.cv <- cross_validation(m, initial = 365, period = 90, horizon = 180, units = 'days')
performance <- performance_metrics(df.cv)
```

## Data Import/Export

### Reading Data

```r
# CSV
data <- read_csv("file.csv")

# Excel
library(readxl)
data <- read_excel("file.xlsx", sheet = "Sheet1")

# Database
library(DBI)
library(RPostgres)

con <- dbConnect(RPostgres::Postgres(),
                 host = "localhost",
                 dbname = "analytics",
                 user = "user",
                 password = "password")

data <- dbGetQuery(con, "SELECT * FROM sales WHERE date >= '2024-01-01'")
dbDisconnect(con)

# JSON
library(jsonlite)
data <- fromJSON("file.json")

# API
library(httr)
response <- GET("https://api.example.com/data")
data <- content(response, as = "parsed")
```

### Writing Data

```r
# CSV
write_csv(data, "output.csv")

# Excel with multiple sheets
library(writexl)
sheets <- list(
  sales = sales_data,
  customers = customer_data,
  summary = summary_data
)
write_xlsx(sheets, "report.xlsx")

# Database
dbWriteTable(con, "new_table", data, overwrite = TRUE)

# RDS (R native format)
saveRDS(data, "data.rds")
data <- readRDS("data.rds")
```

## R Markdown Reports

```r
---
title: "Sales Analysis Report"
author: "PitcanAnalytics"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(knitr)
```

## Executive Summary

```{r summary}
# Calculate key metrics
summary_stats <- data %>%
  summarise(
    total_sales = sum(sales),
    avg_daily_sales = mean(sales),
    growth_rate = (last(sales) - first(sales)) / first(sales) * 100
  )

kable(summary_stats, caption = "Key Performance Indicators")
```

## Sales Trends

```{r plot, fig.width=10, fig.height=6}
ggplot(data, aes(x = date, y = sales)) +
  geom_line(color = "steelblue", size = 1.2) +
  geom_smooth(method = "loess", se = TRUE) +
  labs(title = "Sales Over Time",
       x = "Date",
       y = "Sales ($)") +
  theme_minimal()
```
```

## Performance Optimization

```r
# Use data.table for large datasets
library(data.table)

dt <- as.data.table(data)
result <- dt[age > 18, 
             .(avg_value = mean(value),
               total = sum(value)),
             by = category]

# Parallel processing
library(parallel)
library(foreach)
library(doParallel)

cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

results <- foreach(i = 1:1000, .combine = rbind) %dopar% {
  # Parallel computation
  expensive_function(i)
}

stopCluster(cl)

# Vectorization
# BAD: slow
result <- numeric(length(x))
for(i in seq_along(x)) {
  result[i] <- x[i] * 2 + 1
}

# GOOD: fast
result <- x * 2 + 1

# Use apply family instead of loops
results <- lapply(data_list, function(df) {
  df %>% summarise(mean_value = mean(value))
})
```

## Common Statistical Tests Quick Reference

```r
# Normality test
shapiro.test(data$value)

# Levene's test (homogeneity of variance)
library(car)
leveneTest(value ~ group, data = data)

# Correlation tests
cor.test(data$x, data$y, method = "pearson")
cor.test(data$x, data$y, method = "spearman")

# Multiple comparison correction
p_values <- c(0.01, 0.03, 0.05, 0.08)
p.adjust(p_values, method = "bonferroni")
p.adjust(p_values, method = "BH")  # Benjamini-Hochberg
```

## Package Management

```r
# Install packages
install.packages("tidyverse")
install.packages(c("ggplot2", "dplyr", "tidyr"))

# Install from GitHub
devtools::install_github("username/package")

# Load packages
library(tidyverse)

# Check installed packages
installed.packages()

# Update packages
update.packages(ask = FALSE)
```

## Usage Examples

**Request**: "Analyze sales data with ggplot2"
**Response**: Creates publication-quality visualizations with proper themes

**Request**: "Build a predictive model with tidymodels"
**Response**: Sets up complete ML workflow with tuning and evaluation

**Request**: "Perform time series forecasting"
**Response**: Uses auto.arima or Prophet for forecasting with diagnostics

**Request**: "Create an R Markdown report"
**Response**: Generates reproducible report with code, plots, and tables

## Integration with PitcanAnalytics

- Use R for advanced statistical analysis
- Generate plots for web display (save as PNG/SVG)
- RMarkdown for automated reporting
- Connect to PostgreSQL/MongoDB for data access
- Export results as JSON for API consumption
- Use Shiny for interactive dashboards
